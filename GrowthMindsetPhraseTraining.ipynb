{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GrowthMindsetPhraseTraining.ipynb","provenance":[],"authorship_tag":"ABX9TyOzlg0nWl6s+9cqWiAOasBF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"BNENv6QhEWef","executionInfo":{"status":"error","timestamp":1608231671218,"user_tz":300,"elapsed":4063,"user":{"displayName":"Dr. Jenny Li","photoUrl":"","userId":"07713537177740476435"}},"outputId":"7776fe1d-6e20-42f1-fa96-200d20c9b15b"},"source":["import json\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","DATA_PATH = \"data.json\"\r\n","SAVED_MODEL_PATH = \"model.h5\"\r\n","EPOCHS = 200\r\n","BATCH_SIZE = 5\r\n","PATIENCE = 5\r\n","LEARNING_RATE = 0.0001\r\n","\r\n","\r\n","def load_data(data_path):\r\n","    \"\"\"Loads training dataset from json file.\r\n","\r\n","    :param data_path (str): Path to json file containing data\r\n","    :return X (ndarray): Inputs\r\n","    :return y (ndarray): Targets\r\n","\r\n","    \"\"\"\r\n","    with open(data_path, \"r\") as fp:\r\n","        data = json.load(fp)\r\n","\r\n","    X = np.array(data[\"MFCCs\"])\r\n","    y = np.array(data[\"labels\"])\r\n","    print(\"Training sets loaded!\")\r\n","    return X, y\r\n","\r\n","\r\n","def prepare_dataset(data_path, test_size=0.2, validation_size=0.2):\r\n","    \"\"\"Creates train, validation and test sets.\r\n","\r\n","    :param data_path (str): Path to json file containing data\r\n","    :param test_size (flaot): Percentage of dataset used for testing\r\n","    :param validation_size (float): Percentage of train set used for cross-validation\r\n","\r\n","    :return X_train (ndarray): Inputs for the train set\r\n","    :return y_train (ndarray): Targets for the train set\r\n","    :return X_validation (ndarray): Inputs for the validation set\r\n","    :return y_validation (ndarray): Targets for the validation set\r\n","    :return X_test (ndarray): Inputs for the test set\r\n","    :return X_test (ndarray): Targets for the test set\r\n","    \"\"\"\r\n","\r\n","    # load dataset\r\n","    X, y = load_data(data_path)\r\n","\r\n","    # create train, validation, test split\r\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\r\n","    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\r\n","\r\n","    # add an axis to nd array\r\n","    X_train = X_train[..., np.newaxis]\r\n","    X_test = X_test[..., np.newaxis]\r\n","    X_validation = X_validation[..., np.newaxis]\r\n","\r\n","    return X_train, y_train, X_validation, y_validation, X_test, y_test\r\n","\r\n","\r\n","def build_model(input_shape, loss=\"sparse_categorical_crossentropy\", learning_rate=0.0001):\r\n","    \"\"\"Build neural network using keras.\r\n","\r\n","    :param input_shape (tuple): Shape of array representing a sample train. E.g.: (44, 13, 1)\r\n","    :param loss (str): Loss function to use\r\n","    :param learning_rate (float):\r\n","\r\n","    :return model: TensorFlow model\r\n","    \"\"\"\r\n","\r\n","    # build network architecture using convolutional layers\r\n","    model = tf.keras.models.Sequential()\r\n","\r\n","    # 1st conv layer\r\n","    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape,\r\n","                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\r\n","    model.add(tf.keras.layers.BatchNormalization())\r\n","    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\r\n","\r\n","    # 2nd conv layer\r\n","    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\r\n","                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\r\n","    model.add(tf.keras.layers.BatchNormalization())\r\n","    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\r\n","\r\n","    # 3rd conv layer\r\n","    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu',\r\n","                                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\r\n","    model.add(tf.keras.layers.BatchNormalization())\r\n","    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))\r\n","\r\n","    # flatten output and feed into dense layer\r\n","    model.add(tf.keras.layers.Flatten())\r\n","    model.add(tf.keras.layers.Dense(64, activation='relu'))\r\n","    tf.keras.layers.Dropout(0.3)\r\n","\r\n","    # softmax output layer\r\n","    model.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n","\r\n","    optimiser = tf.optimizers.Adam(learning_rate=learning_rate)\r\n","\r\n","    # compile model\r\n","    model.compile(optimizer=optimiser,\r\n","                  loss=loss,\r\n","                  metrics=[\"accuracy\"])\r\n","\r\n","    # print model parameters on console\r\n","    model.summary()\r\n","\r\n","    return model\r\n","\r\n","\r\n","def train(model, epochs, batch_size, patience, X_train, y_train, X_validation, y_validation):\r\n","    \"\"\"Trains model\r\n","\r\n","    :param epochs (int): Num training epochs\r\n","    :param batch_size (int): Samples per batch\r\n","    :param patience (int): Num epochs to wait before early stop, if there isn't an improvement on accuracy\r\n","    :param X_train (ndarray): Inputs for the train set\r\n","    :param y_train (ndarray): Targets for the train set\r\n","    :param X_validation (ndarray): Inputs for the validation set\r\n","    :param y_validation (ndarray): Targets for the validation set\r\n","\r\n","    :return history: Training history\r\n","    \"\"\"\r\n","\r\n","    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", min_delta=0.001, patience=patience)\r\n","\r\n","    # train model\r\n","    history = model.fit(X_train,\r\n","                        y_train,\r\n","                        epochs=epochs,\r\n","                        batch_size=batch_size,\r\n","                        validation_data=(X_validation, y_validation),\r\n","                        callbacks=[earlystop_callback])\r\n","    return history\r\n","\r\n","\r\n","def plot_history(history):\r\n","    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\r\n","\r\n","    :param history: Training history of model\r\n","    :return:\r\n","    \"\"\"\r\n","\r\n","    fig, axs = plt.subplots(2)\r\n","\r\n","    # create accuracy subplot\r\n","    axs[0].plot(history.history[\"accuracy\"], label=\"accuracy\")\r\n","    axs[0].plot(history.history['val_accuracy'], label=\"val_accuracy\")\r\n","    axs[0].set_ylabel(\"Accuracy\")\r\n","    axs[0].legend(loc=\"lower right\")\r\n","    axs[0].set_title(\"Accuracy evaluation\")\r\n","\r\n","    # create loss subplot\r\n","    axs[1].plot(history.history[\"loss\"], label=\"loss\")\r\n","    axs[1].plot(history.history['val_loss'], label=\"val_loss\")\r\n","    axs[1].set_xlabel(\"Epoch\")\r\n","    axs[1].set_ylabel(\"Loss\")\r\n","    axs[1].legend(loc=\"upper right\")\r\n","    axs[1].set_title(\"Loss evaluation\")\r\n","\r\n","    plt.show()\r\n","\r\n","\r\n","def main():\r\n","    # generate train, validation and test sets\r\n","    X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)\r\n","\r\n","    # create network\r\n","    input_shape = (X_train.shape[1], X_train.shape[2], 1)\r\n","    model = build_model(input_shape, learning_rate=LEARNING_RATE)\r\n","\r\n","    # train network\r\n","    history = train(model, EPOCHS, BATCH_SIZE, PATIENCE, X_train, y_train, X_validation, y_validation)\r\n","\r\n","    # plot accuracy/loss for training/validation set as a function of the epochs\r\n","    plot_history(history)\r\n","\r\n","    # evaluate network on test set\r\n","    test_loss, test_acc = model.evaluate(X_test, y_test)\r\n","    print(\"\\nTest loss: {}, test accuracy: {}\".format(test_loss, 100*test_acc))\r\n","\r\n","    # save model\r\n","    model.save(SAVED_MODEL_PATH)\r\n","\r\n","\r\n","if __name__ == \"__main__\":\r\n","    main()\r\n","\r\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-46ca0b8b8193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-46ca0b8b8193>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;31m# generate train, validation and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# create network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-46ca0b8b8193>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(data_path, test_size, validation_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# create train, validation, test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-46ca0b8b8193>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.json'"]}]},{"cell_type":"code","metadata":{"id":"zMAI_-qGErFW","executionInfo":{"status":"aborted","timestamp":1608231671213,"user_tz":300,"elapsed":4050,"user":{"displayName":"Dr. Jenny Li","photoUrl":"","userId":"07713537177740476435"}}},"source":[""],"execution_count":null,"outputs":[]}]}